{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/silas/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/silas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/silas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/silas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import data_helpers as dh\n",
    "import pickle\n",
    "from textblob import Word\n",
    "from zipfile import ZipFile\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download as nltk_download\n",
    "from text_processing import preprocess_spacy, clean_str\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words=stopwords.words('english')\n",
    "nltk_download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "full_data_path= '/home/silas/final_project/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path= 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data (labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iea climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_climate():\n",
    "    \n",
    "    load = pd.read_json(full_data_path + 'iea_separate.json', 'column')\n",
    "    load.drop(load.columns[2], axis=1)\n",
    "    load['label'] = 1\n",
    "    data = load[['description:', 'label']]\n",
    "    data.columns = ['texts', 'labels']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eu climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eu():\n",
    "    \n",
    "    load = pd.read_csv('Data/EU_climate_change_mitigation_policies_and_measures.csv')\n",
    "    eu_df = load[['Description:text']]\n",
    "    eu_df['label'] = 1\n",
    "    eu_df.columns = ['texts', 'labels']\n",
    "    data = eu_df.dropna()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_climate_bbc():\n",
    "    \"\"\"\n",
    "    Loads climate (iea_separate.json) and bbc (bbc) datasets. See eda_and_merge.ipynb for merge func. \n",
    "    \"\"\"\n",
    "    mgd = pd.read_pickle('Data/climate_bbc.pkl')\n",
    "    return mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eu_bbc():\n",
    "    \"\"\"\n",
    "    Loads eu (see load_eu() above) and bbc (bbc) datasets. See eda_and_merge.ipynb for merge func. \n",
    "    \"\"\"\n",
    "    mgd = pd.read_pickle('Data/eu_bbc.pkl')\n",
    "    return mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_climate_science():\n",
    "    \"\"\"\n",
    "    Loads climate and science (WOS) datasets. See eda_and_merge.ipynb for merge func. \n",
    "    \"\"\"\n",
    "    mgd = pd.read_pickle('Data/climate_science.pkl')\n",
    "    return mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eu_science():\n",
    "    \"\"\"\n",
    "    Loads eu and science (WOS) datasets. \n",
    "    \"\"\"\n",
    "    mgd = pd.read_pickle('Data/eu_science.pkl')\n",
    "    return mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toy and test merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam():\n",
    "\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    df = pd.DataFrame(text_data)\n",
    "    df[0] = pd.Categorical(df[0]).codes\n",
    "    data = pd.DataFrame({'texts': df[1], 'labels': df[0]})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news20\n",
    "- https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_newsgroups():\n",
    "    \n",
    "    categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "    newsgroups = fetch_20newsgroups(categories=categories)\n",
    "    y_true= [newsgroups.target_names[idx] for idx in newsgroups.target]\n",
    "    # y_true = newsgroups.target\n",
    "    data = pd.DataFrame({'texts': newsgroups.data, 'labels':y_true})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "- https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\n",
    "- (description: https://martin-thoma.com/nlp-reuters/)\n",
    "- http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n",
    "# https://martin-thoma.com/nlp-reuters/\n",
    "\n",
    "n_classes = 90\n",
    "labels = reuters.categories()\n",
    "\n",
    "\n",
    "# def load_reuters(config={}):\n",
    "def load_reuters():\n",
    "    \"\"\"\n",
    "    Load the Reuters dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    test = [d for d in documents if d.startswith('test/')]\n",
    "    train = [d for d in documents if d.startswith('training/')]\n",
    "\n",
    "    docs = {}\n",
    "     \n",
    "    train_lst = [reuters.raw(doc_id) for doc_id in train]\n",
    "    test_lst = [reuters.raw(doc_id) for doc_id in test]\n",
    "    text = train_lst + test_lst\n",
    "    \n",
    "    train_labels = [reuters.categories(doc_id)[0] for doc_id in train]\n",
    "    test_labels = [reuters.categories(doc_id)[0] for doc_id in test]\n",
    "    labels = train_labels + test_labels\n",
    "    \n",
    "    data = pd.DataFrame({'texts':text, 'labels':labels})\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBC news\n",
    "- http://mlg.ucd.ie/datasets/bbc.html\n",
    "- http://mlg.ucd.ie/files/datasets/bbc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bbc():\n",
    "    \n",
    "    TRAIN_PATH = '/home/silas/final_project/W266-final-project-/Input/bbc_dataset.csv'\n",
    "    df = pd.read_csv(TRAIN_PATH)\n",
    "    df_bbc = pd.DataFrame({'texts': df.news, 'labels': df.type})\n",
    "\n",
    "    return df_bbc\n",
    "#        print('writing csv flie ...')\n",
    "#        df_bbc.to_csv('../bbc_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web of Science\n",
    "- https://data.mendeley.com/datasets/9rw3vkcfy4/6#file-bac53024-9266-4a46-b9fe-c193dfeb0b7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_science():\n",
    "    file_x = open('../Data/WOS5736/X.txt', 'r')\n",
    "    file_y = open('../Data/WOS5736/YL1.txt', 'r')\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    for i in file_x:\n",
    "        texts.append(i)\n",
    "    for i in file_y:\n",
    "        labels.append(i.replace('\\n',''))\n",
    "    data = pd.DataFrame({'texts':texts, 'labels':labels})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Optional to run:\n",
    "df_climate = load_climate()\n",
    "df_eu = load_eu()\n",
    "df_climate_bbc=load_climate_bbc()\n",
    "df_eu_bbc=load_eu_bbc()\n",
    "df_spam = load_spam()\n",
    "df_newsgroups = load_newsgroups()\n",
    "df_reuters = load_reuters()\n",
    "df_bbc = load_bbc()\n",
    "df_science = load_science()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, stems (requires df \"text\" labeled)\n",
    "    Args\n",
    "        df: dataframe\n",
    "    Returns\n",
    "        df: dataframe with processed text\n",
    "    \"\"\"\n",
    "    filtered_tokens=[]\n",
    "    for doc in df.texts:\n",
    "        token = nltk.word_tokenize(doc)\n",
    "        filtered_tokens_temp=[]\n",
    "        for tok in token:\n",
    "            if tok not in stop_words:\n",
    "                clean = clean_str(tok)\n",
    "                if re.search('[a-zA-Z]', clean):\n",
    "                    filtered_tokens_temp.append(stemmer.stem(clean))\n",
    "        filtered_tokens.append(filtered_tokens_temp)\n",
    "    df['processed_text'] = filtered_tokens    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps_spacy(df):\n",
    "    df['processed_text'] = preprocess_spacy(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing climate dataset...\n",
      "Preprocessing eu dataset...\n",
      "Preprocessing climate_bbc dataset...\n",
      "Preprocessing eu_bbc dataset...\n",
      "Preprocessing spam dataset...\n",
      "Preprocessing newsgroups dataset...\n",
      "Preprocessing reuters dataset...\n",
      "Preprocessing bbc dataset...\n",
      "Preprocessing science dataset...\n"
     ]
    }
   ],
   "source": [
    "df_datasets={}\n",
    "for df,df_name in zip([df_climate,df_eu,df_climate_bbc,df_eu_bbc, df_spam, df_newsgroups,df_reuters,df_bbc,df_science],\n",
    "                      ['climate','eu','climate_bbc','eu_bbc','spam','newsgroups','reuters','bbc','science']):\n",
    "    print(\"Preprocessing {} dataset...\".format(df_name))\n",
    "    dfp=pre_processing_steps_spacy(df).drop_duplicates(subset='processed_text')\n",
    "    # load to datasets to pickle individually into local Data dir for individual dataset loads\n",
    "    dfp.to_pickle(full_data_path + '{}.pkl'.format(df_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading df_datasets to pickle...\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/df_datasets.pickle', 'wb') as handle:\n",
    "    print('loading df_datasets to pickle...')\n",
    "    pickle.dump(df_datasets, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples to load\n",
    "#df_spam = pd.read_pickle(full_data_path + \"spam.pkl\")\n",
    "#df_newsgroups = pd.read_pickle(full_data_path + \"newsgroups.pkl\")\n",
    "#df_reuters = pd.read_pickle(full_data_path + \"reuters.pkl\")\n",
    "#df_bbc = pd.read_pickle(full_data_path + \"bbc.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
