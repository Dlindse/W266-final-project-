{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/silas/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/silas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/silas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/silas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import data_helpers as dh\n",
    "from textblob import Word\n",
    "from zipfile import ZipFile\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download as nltk_download\n",
    "from text_processing import preprocess_spacy, clean_str\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words=stopwords.words('english')\n",
    "nltk_download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "full_path= '/home/silas/final_project/W266-final-project'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data (labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_climate():\n",
    "\n",
    "    # load = pd.read_json(full_path + '/Data/ieapandm.json', 'column')\n",
    "    load = pd.read_json('/home/silas/final_project/ieapandm.json', 'column')\n",
    "    load['label'] = 1\n",
    "    data = load[['text', 'label']]\n",
    "    data.columns = ['texts', 'labels']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam():\n",
    "\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    df = pd.DataFrame(text_data)\n",
    "    df[0] = pd.Categorical(df[0]).codes\n",
    "    data = pd.DataFrame({'texts': df[1], 'labels': df[0]})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news20\n",
    "- https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_newsgroups():\n",
    "    \n",
    "    categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "    newsgroups = fetch_20newsgroups(categories=categories)\n",
    "    y_true= [newsgroups.target_names[idx] for idx in newsgroups.target]\n",
    "    # y_true = newsgroups.target\n",
    "    data = pd.DataFrame({'texts': newsgroups.data, 'labels':y_true})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "- https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\n",
    "- (description: https://martin-thoma.com/nlp-reuters/)\n",
    "- http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n",
    "# https://martin-thoma.com/nlp-reuters/\n",
    "\n",
    "n_classes = 90\n",
    "labels = reuters.categories()\n",
    "\n",
    "\n",
    "# def load_reuters(config={}):\n",
    "def load_reuters():\n",
    "    \"\"\"\n",
    "    Load the Reuters dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    test = [d for d in documents if d.startswith('test/')]\n",
    "    train = [d for d in documents if d.startswith('training/')]\n",
    "\n",
    "    docs = {}\n",
    "     \n",
    "    train_lst = [reuters.raw(doc_id) for doc_id in train]\n",
    "    test_lst = [reuters.raw(doc_id) for doc_id in test]\n",
    "    text = train_lst + test_lst\n",
    "    \n",
    "    train_labels = [reuters.categories(doc_id)[0] for doc_id in train]\n",
    "    test_labels = [reuters.categories(doc_id)[0] for doc_id in test]\n",
    "    labels = train_labels + test_labels\n",
    "    \n",
    "    data = pd.DataFrame({'texts':text, 'labels':labels})\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBC news\n",
    "- http://mlg.ucd.ie/datasets/bbc.html\n",
    "- http://mlg.ucd.ie/files/datasets/bbc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bbc():\n",
    "    \n",
    "    TRAIN_PATH = '/home/silas/final_project/W266-final-project/Input/bbc_dataset.csv'\n",
    "    df = pd.read_csv(TRAIN_PATH)\n",
    "    df_bbc = pd.DataFrame({'texts': df.news, 'labels': df.type})\n",
    "\n",
    "    return df_bbc\n",
    "#        print('writing csv flie ...')\n",
    "#        df_bbc.to_csv('../bbc_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate = load_climate()\n",
    "df_spam = load_spam()\n",
    "df_newsgroups = load_newsgroups()\n",
    "df_reuters = load_reuters()\n",
    "df_bbc = load_bbc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, stems (requires df \"text\" labeled)\n",
    "    Args: dataframe\n",
    "    Returns: dataframe with processed text\n",
    "    \"\"\"\n",
    "    filtered_tokens=[]\n",
    "    for doc in df.texts:\n",
    "        token = nltk.word_tokenize(doc)\n",
    "        filtered_tokens_temp=[]\n",
    "        for tok in token:\n",
    "            if tok not in stop_words:\n",
    "                clean = clean_str(tok)\n",
    "                if re.search('[a-zA-Z]', clean):\n",
    "                    filtered_tokens_temp.append(stemmer.stem(clean))\n",
    "        filtered_tokens.append(filtered_tokens_temp)\n",
    "    df['processed_text'] = filtered_tokens    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps_spacy(df):\n",
    "    texts = preprocess_spacy(df)\n",
    "    df['processed_text'] = texts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/silas/final_project/W266-final-project/Data/climate.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-33858157c970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_processing_steps_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_climate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/silas/final_project/W266-final-project/Data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'climate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol)\u001b[0m\n\u001b[1;32m   2187\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         return to_pickle(self, path, compression=compression,\n\u001b[0;32m-> 2189\u001b[0;31m                          protocol=protocol)\n\u001b[0m\u001b[1;32m   2190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_clipboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, path, compression, protocol)\u001b[0m\n\u001b[1;32m     71\u001b[0m     f, fh = _get_handle(path, 'wb',\n\u001b[1;32m     72\u001b[0m                         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                         is_text=False)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/silas/final_project/W266-final-project/Data/climate.pkl'"
     ]
    }
   ],
   "source": [
    "pre_processing_steps_spacy(df_climate).to_pickle('/home/silas/final_project/W266-final-project/Data/' + '{}.pkl'.format('climate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing spam dataset...\n",
      "Preprocessing newsgroups dataset...\n",
      "Preprocessing reuters dataset...\n",
      "Preprocessing bbc dataset...\n"
     ]
    }
   ],
   "source": [
    "for df,df_name in zip([df_climate, df_spam, df_newsgroups,df_reuters,df_bbc],['climate','spam','newsgroups','reuters','bbc']):\n",
    "    print(\"Preprocessing {} dataset...\".format(df_name))\n",
    "    pre_processing_steps_spacy(df).to_pickle('/home/silas/final_project/W266-final-project/Data/' + '{}.pkl'.format(df_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = pd.read_pickle(\"./spam.pkl\")\n",
    "df_newsgroups = pd.read_pickle(\"./newsgroups.pkl\")\n",
    "df_reuters = pd.read_pickle(\"./reuters.pkl\")\n",
    "df_bbc = pd.read_pickle(\"./bbc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_test_splitter(df,holdout=.2):    \n",
    "    mask = np.random.rand(len(df)) < 1 - holdout\n",
    "    train,test = df[mask],df[~mask]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train, test = train_test_splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
