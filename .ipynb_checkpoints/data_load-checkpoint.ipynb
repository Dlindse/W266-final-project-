{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/silas/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/silas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/silas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/silas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import data_helpers as dh\n",
    "from textblob import Word\n",
    "from zipfile import ZipFile\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download as nltk_download\n",
    "from text_processing import preprocess_spacy, clean_str\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words=stopwords.words('english')\n",
    "nltk_download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "full_data_path= '/home/silas/final_project/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path= '/home/silas/final_project/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data (labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_climate():\n",
    "\n",
    "    # load = pd.read_json(full_path + '/Data/ieapandm.json', 'column')\n",
    "    load = pd.read_json('/home/silas/final_project/Data/ieapandm.json', 'column')\n",
    "    load['label'] = 1\n",
    "    data = load[['text', 'label']]\n",
    "    data.columns = ['texts', 'labels']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam():\n",
    "\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    df = pd.DataFrame(text_data)\n",
    "    df[0] = pd.Categorical(df[0]).codes\n",
    "    data = pd.DataFrame({'texts': df[1], 'labels': df[0]})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news20\n",
    "- https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_newsgroups():\n",
    "    \n",
    "    categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "    newsgroups = fetch_20newsgroups(categories=categories)\n",
    "    y_true= [newsgroups.target_names[idx] for idx in newsgroups.target]\n",
    "    # y_true = newsgroups.target\n",
    "    data = pd.DataFrame({'texts': newsgroups.data, 'labels':y_true})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "- https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\n",
    "- (description: https://martin-thoma.com/nlp-reuters/)\n",
    "- http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n",
    "# https://martin-thoma.com/nlp-reuters/\n",
    "\n",
    "n_classes = 90\n",
    "labels = reuters.categories()\n",
    "\n",
    "\n",
    "# def load_reuters(config={}):\n",
    "def load_reuters():\n",
    "    \"\"\"\n",
    "    Load the Reuters dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    test = [d for d in documents if d.startswith('test/')]\n",
    "    train = [d for d in documents if d.startswith('training/')]\n",
    "\n",
    "    docs = {}\n",
    "     \n",
    "    train_lst = [reuters.raw(doc_id) for doc_id in train]\n",
    "    test_lst = [reuters.raw(doc_id) for doc_id in test]\n",
    "    text = train_lst + test_lst\n",
    "    \n",
    "    train_labels = [reuters.categories(doc_id)[0] for doc_id in train]\n",
    "    test_labels = [reuters.categories(doc_id)[0] for doc_id in test]\n",
    "    labels = train_labels + test_labels\n",
    "    \n",
    "    data = pd.DataFrame({'texts':text, 'labels':labels})\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBC news\n",
    "- http://mlg.ucd.ie/datasets/bbc.html\n",
    "- http://mlg.ucd.ie/files/datasets/bbc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bbc():\n",
    "    \n",
    "    TRAIN_PATH = '/home/silas/final_project/W266-final-project-/Input/bbc_dataset.csv'\n",
    "    df = pd.read_csv(TRAIN_PATH)\n",
    "    df_bbc = pd.DataFrame({'texts': df.news, 'labels': df.type})\n",
    "\n",
    "    return df_bbc\n",
    "#        print('writing csv flie ...')\n",
    "#        df_bbc.to_csv('../bbc_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web of Science\n",
    "- https://data.mendeley.com/datasets/9rw3vkcfy4/6#file-bac53024-9266-4a46-b9fe-c193dfeb0b7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_science():\n",
    "    file_x = open(full_data_path+'WOS5736/X.txt', 'r')\n",
    "    file_y = open(full_data_path+'WOS5736/Y.txt', 'r')\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    for i in file_x:\n",
    "        texts.append(i)\n",
    "    for i in file_y:\n",
    "        labels.append(i.replace('\\n',''))\n",
    "    data = pd.DataFrame({'texts':texts, 'labels':labels})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional to run:\n",
    "df_climate = load_climate()\n",
    "df_spam = load_spam()\n",
    "df_newsgroups = load_newsgroups()\n",
    "df_reuters = load_reuters()\n",
    "df_bbc = load_bbc()\n",
    "df_science = load_science()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, stems (requires df \"text\" labeled)\n",
    "    Args: dataframe\n",
    "    Returns: dataframe with processed text\n",
    "    \"\"\"\n",
    "    filtered_tokens=[]\n",
    "    for doc in df.texts:\n",
    "        token = nltk.word_tokenize(doc)\n",
    "        filtered_tokens_temp=[]\n",
    "        for tok in token:\n",
    "            if tok not in stop_words:\n",
    "                clean = clean_str(tok)\n",
    "                if re.search('[a-zA-Z]', clean):\n",
    "                    filtered_tokens_temp.append(stemmer.stem(clean))\n",
    "        filtered_tokens.append(filtered_tokens_temp)\n",
    "    df['processed_text'] = filtered_tokens    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps_spacy(df):\n",
    "    df['processed_text'] = preprocess_spacy(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing climate dataset...\n",
      "Preprocessing spam dataset...\n",
      "Preprocessing newsgroups dataset...\n",
      "Preprocessing reuters dataset...\n",
      "Preprocessing bbc dataset...\n",
      "Preprocessing science dataset...\n"
     ]
    }
   ],
   "source": [
    "for df,df_name in zip([df_climate, df_spam, df_newsgroups,df_reuters,df_bbc,df_science],['climate','spam','newsgroups','reuters','bbc','science']):\n",
    "    print(\"Preprocessing {} dataset...\".format(df_name))\n",
    "    pre_processing_steps_spacy(df).drop_duplicates(subset='processed_text').to_pickle(full_data_path + '{}.pkl'.format(df_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples to load\n",
    "#df_spam = pd.read_pickle(full_data_path + \"spam.pkl\")\n",
    "#df_newsgroups = pd.read_pickle(full_data_path + \"newsgroups.pkl\")\n",
    "#df_reuters = pd.read_pickle(full_data_path + \"reuters.pkl\")\n",
    "#df_bbc = pd.read_pickle(full_data_path + \"bbc.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
