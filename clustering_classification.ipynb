{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "import textblob\n",
    "import io\n",
    "import nltk\n",
    "\n",
    "import sklearn.cluster as cluster\n",
    "import hdbscan # https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, silhouette_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "# from tensorflow.python.framework import ops\n",
    "# ops.reset_default_graph()\n",
    "\n",
    "%matplotlib inline\n",
    "# sns.set_context('poster')\n",
    "# sns.set_color_codes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.birch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- inspect data (ex: df.head(), df.groupby('labels').count())\n",
    "- select data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(full_path_data_store +'df_datasets.pickle', 'rb') as handle:\n",
    "    df_datasets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataframe into text and labels\n",
    "# df = df_merged\n",
    "def pre_split_data(df):\n",
    "    texts = df.texts\n",
    "    labels = df.labels\n",
    "    processed_texts = df.processed_text\n",
    "    return  texts,labels,processed_texts\n",
    "\n",
    "\n",
    "texts,labels,processed_texts=pre_split_data(df_datasets[df_name_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNPROCESSED: raw text, split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y_text_label, valid_y_text_label = train_test_split(texts, labels)\n",
    "\n",
    "# PROCESSED: preprocessed text, split the dataset into training and validation datasets \n",
    "train_xp, valid_xp, train_yp_text_label, valid_yp_text_label = train_test_split(processed_texts, labels)\n",
    "\n",
    "# label encode the target variable for raw and processed datasets\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y_text_label)\n",
    "valid_y = encoder.fit_transform(valid_y_text_label)\n",
    "train_yp = encoder.fit_transform(train_yp_text_label)\n",
    "valid_yp = encoder.fit_transform(valid_yp_text_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize data labels\n",
    "- for binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if binary experiment\n",
    "\n",
    "binary=input('enter 1 to binarize ')\n",
    "\n",
    "def binarize():\n",
    "    \n",
    "    train_y = np.array((pd.DataFrame({'t_label':train_y}).t_label == 1).astype('int'))\n",
    "    valid_y = np.array((pd.DataFrame({'t_label':valid_y}).t_label == 1).astype('int'))\n",
    "    train_yp = np.array((pd.DataFrame({'t_label':train_yp}).t_label == 1).astype('int'))\n",
    "    valid_yp = np.array((pd.DataFrame({'t_label':valid_yp}).t_label == 1).astype('int'))\n",
    "    \n",
    "    return train_y,valid_y,train_yp,valid_yp\n",
    "\n",
    "if binary==1:\n",
    "    train_y,valid_y,train_yp,valid_yp=binarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=20, ngram_range=(1,1), tokenizer=None, \n",
    "                               lowercase=True, stop_words='english',analyzer= \"word\", norm='l2')\n",
    "xtrain_vector = vectorizer.fit_transform(train_x)\n",
    "# print(vector.shape)\n",
    "# print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(texts)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vector Space\n",
    "\n",
    "#### Tfidf vectorizer:\n",
    "- Strips out “stop words”\n",
    "- Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "- Filters out terms that occur in only one document (min_df=2).\n",
    "- Selects the 10,000 most frequently occuring words in the corpus.\n",
    "- Normalizes the vector (L2 norm of 1.0) to normalize the effect of document length on the tf-idf values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=2, \n",
    "                             stop_words='english', use_idf=True, analyzer='word')\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# token_pattern=r'\\w{1,}', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=2, \n",
    "                             stop_words='english', use_idf=True, analyzer='word', ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(texts)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=2, \n",
    "                             stop_words='english', use_idf=True, analyzer='char', ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(texts)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models\n",
    "- https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "- from gensim.test.utils import common_corpus, common_dictionary\n",
    "- lda = LdaMulticore(common_corpus, id2word=common_dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, ensemble\n",
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "# https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "# https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#remove proper names\n",
    "preprocess = [strip_proppers_POS(doc) for doc in texts]\n",
    "# %time tokenized_text = [tokenizer(text) for text in preprocess]\n",
    "\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in preprocess]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "c lda = models.LdaModel(corpus, num_topics=20, id2word=dictionary, update_every=5, chunksize=10000, passes=100)\n",
    "lda.print_topics(20, num_words=20)\n",
    "lda.show_topics(formatted=False, num_words=20)\n",
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "lda.get_topics().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "- multi classification problem without labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_score(cluster_labels,labels=train_y):\n",
    "    print(\"Homogeneity: {:.2f}\".format(homogeneity_score(labels, cluster_labels)))\n",
    "    print(\"Completeness: %0.3f\" % completeness_score(labels, cluster_labels))\n",
    "    print(\"V-measure: %0.3f\" % v_measure_score(labels, cluster_labels))\n",
    "    print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % adjusted_rand_score(labels, cluster_labels))\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % silhouette_score(data, cluster_labels, sample_size=1000))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between documents\n",
    "%time dist = 1 - cosine_similarity(xtrain_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrary to PCA, SVD estimator does not center the data before computing the singular value decomposition. \n",
    "# This means it can work with scipy.sparse matrices efficiently\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "%time pos_svd = svd.fit_transform(dist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "pca = RandomizedPCA(n_components=20)\n",
    "pos_pca = pca.fit_transform(dist)\n",
    "# x_pca, y_pca = pos_pca[:,0],pos_pca[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "%time X_embedded = TSNE(n_components=2).fit_transform(pos_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS is memory consumptive, and takes time if distance matrix is greater than 700x700\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "# x_mds, y_mds = pos[:, 0], pos[:, 1]\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded.shape\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "A = Normalizer().fit_transform(pos_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html\n",
    "# data =  A\n",
    "import sklearn.cluster as cluster\n",
    "\n",
    "\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "\n",
    "def plot_clusters(data, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    end_time = time.time()\n",
    "    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)\n",
    "    plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)\n",
    "    cluster_score(labels,train_y)\n",
    "    print(classification_report(labels,train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gcv = GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=50, shuffle=True),\n",
    "       error_score='raise',\n",
    "       estimator=cluster.KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "    n_clusters=8, n_init=10, n_jobs=1, precompute_distances='auto',\n",
    "    random_state=None, tol=0.0001, verbose=0),\n",
    "       fit_params=None, iid=True, n_jobs=-1,\n",
    "       param_grid={'max_iter': [100, 200, 300, 400, 500], 'n_init': [10, 15, 20], 'tol': [1e-07, 1e-06, 1e-05, 0.0001], 'n_clusters': [2, 3]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
    "       scoring=None, verbose=0)\n",
    "gcv.fit(data)     \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5ac56c43b286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_clusters'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_topics' is not defined"
     ]
    }
   ],
   "source": [
    "plot_clusters(X_topics, cluster.KMeans, (), {'n_clusters':16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.MiniBatchKMeans, (), {'n_clusters':2, 'init':'k-means++', 'n_init':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.AffinityPropagation, (), {'preference':-5.0, 'damping':0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.MeanShift, (0.175,), {'cluster_all':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.SpectralClustering, (), {'n_clusters':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.AgglomerativeClustering, (), {'n_clusters':16, 'linkage':'ward'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.DBSCAN, (), {'eps':0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, hdbscan.HDBSCAN, (), {'min_cluster_size':20, 'min_samples':25})\n",
    "\n",
    "# prediction:\n",
    "#test_labels, strengths = hdbscan.approximate_predict(clusterer, test_points)\n",
    "#test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering_functions import *\n",
    "from text_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_matrix(content_as_str):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=0.2,\n",
    "                                       stop_words='english',use_idf=True,\n",
    "                                       tokenizer=tokenizer, ngram_range=(1,3))\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(content_as_str) #fit the vectorizer to synopses\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return (similarity_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,tfidf_matrix = get_similarity_matrix(train_x[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html\n",
    "    \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# we only want to keep the body of the documents!\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# fetch train and test data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "\n",
    "# a list of 18,846 cleaned news in string format\n",
    "# only keep letters & make them all lower case\n",
    "news = [' '.join(filter(str.isalpha, raw.lower().split())) for raw in\n",
    "        newsgroups_train.data + newsgroups_test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import decomposition, ensemble\n",
    "# train a LDA Model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_topics = 20 # number of topics\n",
    "n_iter = 500 # number of iterations\n",
    "\n",
    "# vectorizer: ignore English stopwords & words that occur less than 5 times\n",
    "cvectorizer = CountVectorizer(min_df=5, stop_words='english')\n",
    "cvz = cvectorizer.fit_transform(news)\n",
    "\n",
    "# train an LDA model\n",
    "# lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lda_keys = []\n",
    "for i in range(X_topics.shape[0]):\n",
    "      _lda_keys.append(X_topics[i].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "threshold = 0.5\n",
    "_idx = np.amax(X_topics, axis=1) > threshold  # idx of doc that above the threshold\n",
    "X_topics = X_topics[_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# a t-SNE model\n",
    "# angle value close to 1 means sacrificing accuracy for speed\n",
    "# pca initializtion usually leads to better results \n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "\n",
    "# 20-D -> 2-D\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import save\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "n_top_words = 5 # number of keywords we show\n",
    "\n",
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "topic_words={}\n",
    "for topic, comp in enumerate(lda_model.components_):\n",
    "    # for the n-dimensional array \"arr\":\n",
    "    # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n",
    "    # which contains the indices that would sort arr in a descending fashion\n",
    "    # for the ith element in ranked_array, ranked_array[i] represents the index of the\n",
    "    # element in arr that should be at the ith index in ranked_array\n",
    "    # ex. arr = [3,7,1,0,3,6]\n",
    "    # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
    "    # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n",
    "    # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "\n",
    "    # store the words most relevant to the topic\n",
    "    topic_words[topic] = [X_topics[i] for i in word_idx]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "topic_word = lda_model.topic_word_  # all topic words # np.array(list(topic_words.keys())) #\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "  topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1] # get!\n",
    "  topic_summaries.append(' '.join(topic_words)) # append!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = '20 newsgroups LDA viz'\n",
    "num_example = len(X_topics)\n",
    "\n",
    "plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n",
    "                     title=title,\n",
    "                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n",
    "                 color=colormap[_lda_keys][:num_example])\n",
    "\n",
    "source=bp.ColumnDataSource(data={\n",
    "                   \"content\": news[:num_example],\n",
    "                   \"topic_key\": _lda_keys[:num_example],\n",
    "                   })\n",
    "\n",
    "plot_lda.circle(x='content',y='topic_key', source=source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_coord = np.empty((X_topics.shape[1], 2)) * np.nan\n",
    "for topic_num in _lda_keys:\n",
    "    if not np.isnan(topic_coord).any():\n",
    "        break\n",
    "    topic_coord[topic_num] = tsne_lda[_lda_keys.index(topic_num)]\n",
    "\n",
    "# plot crucial words\n",
    "for i in range(X_topics.shape[1]):\n",
    "    plot_lda.text(topic_coord[i, 0], topic_coord[i, 1], [topic_summaries[i]])\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"topic_key\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save the plot\n",
    "save(plot_lda, '{}.html'.format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "show(plot_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(filename='/home/silas/final_project/W266-final-project/20 newsgroups LDA viz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = '20 newsgroups LDA viz'\n",
    "num_example = len(X_topics)\n",
    "\n",
    "plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n",
    "                     title=title,\n",
    "                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n",
    "                 color=colormap[_lda_keys][:num_example],\n",
    "                 source=bp.ColumnDataSource({\n",
    "                   \"content\": news[:num_example],\n",
    "                   \"topic_key\": _lda_keys[:num_example]\n",
    "                   }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose a news (within a topic) coordinate as the crucial words coordinate\n",
    "topic_coord = np.empty((X_topics.shape[1], 2)) * np.nan\n",
    "for topic_num in _lda_keys:\n",
    "  if not np.isnan(topic_coord).any():\n",
    "    break\n",
    "  topic_coord[topic_num] = tsne_lda[_lda_keys.index(topic_num)]\n",
    "\n",
    "# plot crucial words\n",
    "for i in xrange(X_topics.shape[1]):\n",
    "  plot_lda.text(topic_coord[i, 0], topic_coord[i, 1], [topic_summaries[i]])\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "# save the plot\n",
    "save(plot_lda, '{}.html'.format(title))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
