{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import RandomizedPCA\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import urllib3 as urllib2\n",
    "import google_compute_engine\n",
    "import boto\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import mpld3\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 ms, sys: 265 µs, total: 13.9 ms\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%time pubs = spark.read.json(\"/home/silas/final_project/pubs_data/s2-corpus-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- doiUrl: string (nullable = true)\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- journalPages: string (nullable = true)\n",
      " |-- journalVolume: string (nullable = true)\n",
      " |-- outCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- paperAbstract: string (nullable = true)\n",
      " |-- pdfUrls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- s2PdfUrl: string (nullable = true)\n",
      " |-- s2Url: string (nullable = true)\n",
      " |-- sources: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|       paperAbstract|\n",
      "+--------------------+\n",
      "|The marsh frog is...|\n",
      "|                    |\n",
      "|                    |\n",
      "|The case records ...|\n",
      "|Freezing artifact...|\n",
      "|Gammadelta T cell...|\n",
      "|                    |\n",
      "|                    |\n",
      "|Traditionally, ra...|\n",
      "|In war-time or in...|\n",
      "|Here, we show tha...|\n",
      "|The present study...|\n",
      "|                    |\n",
      "|I compare the rel...|\n",
      "|Compassion fatigu...|\n",
      "|This study was un...|\n",
      "|Low-grade (LG) se...|\n",
      "|                    |\n",
      "|                    |\n",
      "|Genetic polymorph...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pubs.printSchema()\n",
    "pubs.registerTempTable(\"pubs\")\n",
    "#sqlContext.sql(\"select paperAbstract from pubs\").show()\n",
    "pubs.select(\"paperAbstract\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(authors=[Row(ids=['2506899'], name='M. V. Ushakov')], doi='10.1023/A:1020916001559', doiUrl='https://doi.org/10.1023/A:1020916001559', entities=['Amphibians', 'Anura', 'Apache Gora', 'Aquatic ecosystem', 'Diazooxonorleucine', 'Habitat', 'Human body', 'Natural Selection', 'Natural Springs', 'Population', 'Rana esculenta', 'Rana temporaria'], id='7e58b926bbbc122edeccb7cb4f7f68ca11480698', inCitations=[], journalName='Russian Journal of Ecology', journalPages='446-451', journalVolume='33', outCitations=[], paperAbstract='The marsh frog is a widespread and flexible species that mainly occupies various aquatic biotopes. In the Lipetsk oblast, these frogs avoid only closed forest water bodies and springs, and their habitats in the Central Russian Upland and the Oka–Don Lowland obviously differ from each other. According to Klimov et al. (1999), the number of these amphibians in the Oka– Don Lowland is greater. The comparison of morphological variation in frogs from these regions shows that the pressure of natural selection is greater in the Central Russian Upland (Vykhodtseva, 1992; Vykhodtseva and Klimov, 1993; Kovylina and Vykhodtseva, 1993; Klimov et al. , 1999), and this pressure determines the relationship between the demographic and morphological characteristics of the amphibian populations.', pdfUrls=[], pmid='', s2PdfUrl='', s2Url='https://semanticscholar.org/paper/7e58b926bbbc122edeccb7cb4f7f68ca11480698', sources=[], title=\"Ecomorphological Characteristics of the Marsh Frog Rana ridibunda from the Galich'ya Gora Nature Reserve\", venue='Russian Journal of Ecology', year=2004)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lst = pubs.sample(fraction=0.05, seed=3).select(\"title\",\"paperAbstract\",\"entities\").limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_pd_df = abs_lst.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This study was undertaken to determine whether there was an association between fine particle (PM₂.₅) levels and daily clinic visits for migraine in Taipei, Taiwan. Daily clinic visits for migraine and ambient air pollution data for Taipei were obtained for the period from 2006-2011. The odds ratio of clinic visits was estimated using a case-crossover approach, controlling for weather variables, day of the week, seasonality, and long-term time trends. Generally, no significant associations between PM₂.₅ levels and migraine visits were observed on cool days. On warm days, however, for the single pollutant model (without adjustment for other pollutants), increased clinic visits for migraine were significantly associated with PM₂.₅ levels, with an interquartile range (IQR) rise associated with a 13% (95% CI = 8%-19%) elevation in number of migraine visits. In bi-pollutant model, PM₂.₅ remained significant after the inclusion of sulfur dioxide (SO₂) or ozone (O₃) on warm days. This study provides evidence that higher levels of PM₂.₅ increase the risk of clinic visits for migraine in Taipei, Taiwan.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note missing paper abstracts\n",
    "abs_pd_df.paperAbstract[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get records with paper abstracts data completed\n",
    "# df = sqlContext.createDataFrame(rdd, [\"user_id\", \"object_id\", \"score\"]\n",
    "abstracts = abs_pd_df[abs_pd_df.paperAbstract != ''].paperAbstract\n",
    "titles = abs_pd_df[abs_pd_df.paperAbstract != ''].title\n",
    "entities = abs_pd_df[abs_pd_df.paperAbstract != ''].entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
    "ranks = []\n",
    "\n",
    "for i in range(0,len(abstracts)):\n",
    "    ranks.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_step(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns totalvocab_tokenized and totalvocab_stemmed\n",
    "    \"\"\"\n",
    "\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in text:\n",
    "        allwords_stemmed = tokenize_and_stem(i)\n",
    "        totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "        allwords_tokenized = tokenize_only(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "    return totalvocab_tokenized, totalvocab_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in abstracts:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thisf-idf ttells how important a word is to a document in a collection or corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(abstracts)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each term is notionally assigned a different dimension and a document is characterised \n",
    "# by a vector where the value in each dimension corresponds to \n",
    "# the number of times the term appears in the document. \n",
    "# Cosine similarity then gives a useful measure of how similar two documents are likely to be \n",
    "# in terms of their subject matter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans takes the tf-idf matrix and computes clusters/labels and centers\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# joblib.dump(km,  'doc_cluster.pkl')\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist() # length e.g. 1 for each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = { 'title': titles, 'rank': ranks, 'abstracts': abstracts, 'clusters': clusters, 'entities': entities }\n",
    "\n",
    "frame = pd.DataFrame(features, columns = ['rank', 'title', 'clusters', 'entities'])\n",
    "\n",
    "#frame = pd.DataFrame({'rank':ranks, 'title': titles, 'entities':entities, 'cluster':clusters})\n",
    "\n",
    "frame['clusters'].value_counts()\n",
    "frame['cluster'] = frame['clusters']\n",
    "frame = frame.set_index('cluster')\n",
    "frame[['rank', 'title', 'clusters', 'entities']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i make titles for the clusters by term frequencies\n",
    "final_title_lst=[]\n",
    "for i in range(len(set(frame.clusters))):\n",
    "    texts = list(frame[frame.clusters == i].title)\n",
    "    #texts = ['hi there', 'hello there', 'hello here you are']\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    freq = np.ravel(X.sum(axis=0))\n",
    "    # get vocabulary keys, sorted by value\n",
    "    vocab = [v[0] for v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]\n",
    "    fdist = dict(zip(vocab, freq)) # return same format as nltk\n",
    "    fdist_sort = sorted(fdist.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    temp_lst=[]\n",
    "    for i,j in enumerate(fdist_sort):\n",
    "        if j[0] not in stopwords and j[1] > 2 and j[1] < 6:    # remove stopwords and get terms that aren't too frequent \n",
    "            temp_lst.append(j[0])\n",
    "            #print(temp_lst)\n",
    "        \n",
    "            \n",
    "    final_title_lst.append(temp_lst[2:6])  # select n terms for cluster titles\n",
    "    \n",
    "    \n",
    "    \n",
    "final_title_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame['rank'].groupby(frame['clusters'])\n",
    "\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame.loc[terms[2].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster {} words:\".format(i), end='')\n",
    "    for ind in order_centroids[i, :num_clusters+1]:\n",
    "        print(' {}'.format(vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore')), end=',')\n",
    "    print(\"\")\n",
    "    print()\n",
    "    print(\"Cluster {} titles:\".format(i), end='')\n",
    "    for title in frame.loc[i]['title'].head().values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['Rank'] = frame['rank'] + 1\n",
    "frame['Title'] = frame['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frame[['Rank', 'Title']].loc[frame['clusters'] == 1].to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "x_mds, y_mds = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pos_pca = pca.fit_transform(dist)\n",
    "x_pca, y_pca = pos_pca[:,0],pos_pca[:,1]\n",
    "# print(pca.explained_variance_ratio_)  \n",
    "# print(pca.singular_values_)  \n",
    "#plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "#plt.xlabel('n components')\n",
    "#plt.ylabel('cumulative variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e',\n",
    "                  5: '#d85f02', 6: '#6570b3', 7: '#e6298a', 8: '#56a61e', 9: '#e9a61e'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts= {}\n",
    "for i,j in enumerate(final_title_lst):\n",
    "    dicts[i] = \", \".join(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Family, home, war', \n",
    "                 1: 'Police, killed, murders', \n",
    "                 2: 'Father, New York, brothers', \n",
    "                 3: 'Dance, singing, love', \n",
    "                 4: 'Killed, soldiers, captain'}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data for plots to MDS or PCA transformed\n",
    "x,y=x_mds,y_mds\n",
    "# x,y=x_pca,y_pca     # unhash for PCA transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def \n",
    "\n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=x, y=y, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 15)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'][:20], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=x, y=y, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 15)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'][:20], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples for plotting\n",
    "\n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=x, y=y, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster, with options to sample \n",
    "# n = 300  \n",
    "#groups = df.sample(n).groupby('label')\n",
    "groups = df.groupby('label')\n",
    "\n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(14,10)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, label=cluster_names[name], mec='none', color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1,loc='lower left') #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot\n",
    "\n",
    "#uncomment the below to export to html\n",
    "# html = mpld3.fig_to_html(fig)\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "n = 50 # number of abstracts to structure\n",
    "idx = np.random.choice(dist.shape[0], size=n, replace=False)\n",
    "\n",
    "linkage_matrix = ward(dist[idx]) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=list(titles)[:100]);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False)\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip any proper names from a text...unfortunately right now this is yanking the first word from a sentence too.\n",
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "# https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "# https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#remove proper names\n",
    "preprocess = [strip_proppers(doc) for doc in abstracts]\n",
    "\n",
    "%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n",
    "\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "%time lda = models.LdaModel(corpus, num_topics=20, id2word=dictionary, update_every=5, chunksize=10000, passes=100)\n",
    "\n",
    "lda.print_topics(20, num_words=20)\n",
    "\n",
    "lda.show_topics(formatted=False, num_words=20)\n",
    "\n",
    "lda.get_topics().shape\n",
    "\n",
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans requires turning the test doc to tfidf_matrix\n",
    "km.predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
