{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET PDFS FROM SEARCH QUERY (GOOGLE): CONVERT PDFS TO TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from os import listdir, getcwd\n",
    "from os.path import isfile, join\n",
    "import certifi\n",
    "import urllib3\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tika\n",
    "from tika import parser\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from IPython.display import clear_output\n",
    "import nltk.data\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "- Get links by search term: search for links to pdfs matching search term\n",
    "- Get pdfs by url: using links, download pdfs into local directory\n",
    "- Get text from pdfs: parse pdfs from local directory to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET PDF LINKS BY SEARCH TERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_links(search_term, num_results):\n",
    "    \n",
    "    '''gets pdf links from google based on search term'''\n",
    "    \n",
    "    page = requests.get(\"https://www.google.com/search?q={}+filetype%3A+pdf&num={}\".format(\n",
    "                    search_term, num_results))\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    # links = soup.findAll(\"a\", href=True)\n",
    "    links = soup.findAll('a', attrs={'href': re.compile(\"\\.pdf\")})\n",
    "    pdf_urls = [str(link).split('/url?q=',1)[1].split('.pdf')[0]+'.pdf' \n",
    "                for link in links if '/search' not in str(link)]\n",
    "    \n",
    "    return pdf_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET PDFS BY URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_from_web(url):\n",
    "    \n",
    "    '''\n",
    "    takes a single url and downloads pdfs to pdf_dir\n",
    "    '''\n",
    "    \n",
    "    chunk_size = 100\n",
    "    file = url.split('/')[-1]\n",
    "    path = getcwd() + '/pdf_dir/' + file\n",
    "\n",
    "    # http = urllib3.PoolManager()\n",
    "    \n",
    "    http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "    \n",
    "    r = http.request('GET', url, preload_content=False)\n",
    "\n",
    "    with open(path, 'wb') as out:\n",
    "        while True:\n",
    "            data = r.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            out.write(data)\n",
    "\n",
    "    r.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feeder\n",
    "\n",
    "def web_dir_pdf_feeder(pdf_urls):\n",
    "    \n",
    "    '''takes a list of urls and passes them to get_pdf function to download pdfs from web'''\n",
    "    \n",
    "    max_count = len(pdf_urls)\n",
    "    f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    print('downloading {} pdfs...'.format(max_count))\n",
    "    display(f) # display the bar\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for pdf in pdf_urls:\n",
    "        \n",
    "        \n",
    "        f.value+=1\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "        try:\n",
    "            get_pdf_from_web(pdf)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT PDF TO TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_dir(mypath):\n",
    "    \n",
    "    '''args: my path - default is cwd\n",
    "    function returns list of pdf file names from directory'''\n",
    "    \n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    pdf_file_names = [file for file in onlyfiles if '.pdf' in file]\n",
    "    # file = \"/\"+ pdf_file_names # here to change to iterate through a directory of pdfs\n",
    "    # fp=open(mypath+file,'rb');\n",
    "    return pdf_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2text(path):\n",
    "# tika.initVM()\n",
    "\n",
    "    files = get_files_from_dir(path)\n",
    "    max_count = len(files)\n",
    "    print(\"processing {} pdfs...\".format(max_count))\n",
    "\n",
    "    dict_meta_pdf={}\n",
    "    dict_content_pdf={}\n",
    "    sentence=[]\n",
    "    sentences=[]\n",
    "    \n",
    "    f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    \n",
    "    display(f) # display the bar\n",
    "\n",
    "    for idx,file in enumerate(files):\n",
    "    \n",
    "        # for progress bar\n",
    "        max_count = len(files)\n",
    "        f.value+=1\n",
    "    \n",
    "        # set path to file\n",
    "        full_path = path + \"/\" + file\n",
    "        \n",
    "        try:\n",
    "            parsed = parser.from_file(full_path)\n",
    "        except ConnectionError:\n",
    "            sleep(3)\n",
    "        # parsed = parser.from_file(full_path)\n",
    "        if parsed['status'] != 422:\n",
    "            # dict_meta_pdf[idx]= parsed[\"metadata\"]\n",
    "            content = re.findall(r\"[\\w']+|[.,!?;]\", parsed[\"content\"])\n",
    "    \n",
    "        for word in content:\n",
    "            if word != '.':\n",
    "                sentence.append(word)\n",
    "                content = []\n",
    "\n",
    "            elif len(sentence)>10:\n",
    "                sentences.append(\" \".join(sentence + ['.']).replace(' .','.').replace(' ,', ',').replace(' ?','?'))\n",
    "                sentence=[]\n",
    "            \n",
    "        dict_content_pdf[idx] = sentences\n",
    "        sentences = []\n",
    "    \n",
    "    dict_={}\n",
    "    for i in range(len(dict_content_pdf)):\n",
    "        dict_[i] = pd.Series(\" \".join(dict_content_pdf[i]))\n",
    "\n",
    "    pdf_texts = pd.DataFrame.from_dict(dict_, orient='index')[0]\n",
    "\n",
    "    return pdf_texts, files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'climate change'\n",
    "num_results = 100\n",
    "pdf_directory = '/pdf_dir'\n",
    "path = getcwd() + pdf_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 2.63 ms, total: 2.52 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%time pdf_urls = get_links(search_term, num_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 53 pdfs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6383b8bfe849a5b5b7c437142576a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=53)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ftp'\n",
      "CPU times: user 14 s, sys: 897 ms, total: 14.9 s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%time web_dir_pdf_feeder(pdf_urls) # download pdfs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 47 pdfs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8449fcc826a4232bf481b6106f77fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=47)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 17:51:35,290 [MainThread  ] [WARNI]  Tika server returned status: 422\n",
      "2018-11-12 17:51:43,806 [MainThread  ] [WARNI]  Tika server returned status: 422\n",
      "2018-11-12 17:51:45,981 [MainThread  ] [WARNI]  Tika server returned status: 422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.98 s, sys: 368 ms, total: 2.35 s\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%time pdf_texts, files = pdf2text(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [len(i.split()) for i in pdf_texts]\n",
    "df = pd.DataFrame({'name':files,'content':pdf_texts, 'word_count':words})\n",
    "df.to_pickle('../Data/pdfs_{}.pkl'.format(search_term))\n",
    "# pd.read_pickle('save_content.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
