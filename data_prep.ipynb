{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/silas/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/silas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/silas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/silas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_helpers as dh\n",
    "from textblob import Word\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download as nltk_download\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words=stopwords.words('english')\n",
    "nltk_download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data (labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news20\n",
    "- https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_newsgroups():\n",
    "    \n",
    "    categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "    newsgroups = fetch_20newsgroups(categories=categories)\n",
    "    y_true= [newsgroups.target_names[idx] for idx in newsgroups.target]\n",
    "    # y_true = newsgroups.target\n",
    "    data = pd.DataFrame({'text': newsgroups.data, 'labels':y_true})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "- https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\n",
    "- (description: https://martin-thoma.com/nlp-reuters/)\n",
    "- http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n",
    "# https://martin-thoma.com/nlp-reuters/\n",
    "\n",
    "n_classes = 90\n",
    "labels = reuters.categories()\n",
    "\n",
    "\n",
    "# def load_reuters(config={}):\n",
    "def load_reuters():\n",
    "    \"\"\"\n",
    "    Load the Reuters dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    test = [d for d in documents if d.startswith('test/')]\n",
    "    train = [d for d in documents if d.startswith('training/')]\n",
    "\n",
    "    docs = {}\n",
    "     \n",
    "    train_lst = [reuters.raw(doc_id) for doc_id in train]\n",
    "    test_lst = [reuters.raw(doc_id) for doc_id in test]\n",
    "    text = train_lst + test_lst\n",
    "    \n",
    "    train_labels = [reuters.categories(doc_id) for doc_id in train]\n",
    "    test_labels = [reuters.categories(doc_id) for doc_id in test]\n",
    "    labels = train_labels + test_labels\n",
    "    \n",
    "    data = pd.DataFrame({'text':text, 'labels':labels})\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBC news\n",
    "- http://mlg.ucd.ie/datasets/bbc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bbc():\n",
    "    \n",
    "    os.chdir('/home/silas/MIDS/W266/W266-final-project/')\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(cwd+\"/Input/bbc\")\n",
    "    folders = [\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"]\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in folders:\n",
    "        files = os.listdir(i)\n",
    "        for text_file in files:\n",
    "            file_path = i + \"/\" +text_file\n",
    "            # print(\"reading file:\", file_path)\n",
    "            with open(file_path, 'r') as f:\n",
    "                try:\n",
    "                    data = f.readlines()\n",
    "                except:\n",
    "                    pass\n",
    "            data = ' '.join(data)\n",
    "            x.append(data)\n",
    "            y.append(i)\n",
    "   \n",
    "\n",
    "    df_bbc = pd.DataFrame({'text': x, 'label': y})\n",
    "\n",
    "    return df_bbc\n",
    "#        print('writing csv flie ...')\n",
    "#        df_bbc.to_csv('../bbc_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/silas/MIDS/W266/W266-final-project/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f6e9dd97ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_newsgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_reuters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_reuters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_bbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_bbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4f38d58c7779>\u001b[0m in \u001b[0;36mload_bbc\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_bbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/silas/MIDS/W266/W266-final-project/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/Input/bbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/silas/MIDS/W266/W266-final-project/'"
     ]
    }
   ],
   "source": [
    "df_newsgroups = load_newsgroups()\n",
    "df_reuters = load_reuters()\n",
    "df_bbc = load_bbc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_steps(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenizes, removes stop words, stems (requires df \"text\" labeled)\n",
    "    Args: dataframe\n",
    "    Returns: dataframe with processed text\n",
    "    \"\"\"\n",
    "    filtered_tokens=[]\n",
    "    for doc in df.text:\n",
    "        token = nltk.word_tokenize(doc)\n",
    "        filtered_tokens_temp=[]\n",
    "        for tok in token:\n",
    "            if tok not in stop_words:\n",
    "                clean = clean_str(tok)\n",
    "                if re.search('[a-zA-Z]', clean):\n",
    "                    filtered_tokens_temp.append(stemmer.stem(clean))\n",
    "        filtered_tokens.append(\", \".join(filtered_tokens_temp))\n",
    "    df['processed_text'] = filtered_tokens    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,df_name in zip([df_newsgroups,df_reuters,df_bbc],['newsgroups','reuters','bbc']):\n",
    "    pre_processing_steps(df).to_pickle(\"./{}.pkl\".format(df_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newsgroups = pd.read_pickle(\"./newsgroups.pkl\")\n",
    "df_reuters = pd.read_pickle(\"./reuters.pkl\")\n",
    "df_bbc = pd.read_pickle(\"./bbc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(df,holdout=.2):    \n",
    "    mask = np.random.rand(len(df)) < 1 - holdout\n",
    "    train,test = df[mask],df[~mask]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
